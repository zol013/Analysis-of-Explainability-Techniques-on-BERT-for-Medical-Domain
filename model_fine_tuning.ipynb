{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vI7sTjSLBv00"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NgHZZt-B9TW"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glHkoWmoiAV7"},"outputs":[],"source":["import re\n","import json\n","import torch\n","import string\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","from tqdm.auto import tqdm\n","from matplotlib import pyplot as plt\n","from transformers import get_scheduler\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","num_classes = 2\n","num_epochs = 101\n","batch_size = 32\n","hidden_dim = 256\n","max_length = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TEoHozjiAV_"},"outputs":[],"source":["class MedicalTCDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data['medical_abstract']\n","        self.labels = data['condition_label']\n","    def __len__(self):\n","        return len(self.data)\n","    def __getitem__(self, index):\n","        return self.data[index], self.labels[index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exTD_Il4iAWA"},"outputs":[],"source":["def clean_medical_text(text):\n","    text = text.lower()\n","    text = re.sub('[' + string.punctuation + ']', '', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    stopwords = ['the', 'a', 'an', 'in', 'on', 'is', 'are', 'was', 'were', 'to', 'for', 'of']\n","    text = ' '.join(word for word in text.split() if word not in stopwords)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"detUiPyviAWB"},"outputs":[],"source":["train = pd.read_csv(\"/content/gdrive/MyDrive/XAI/data/medical_tc_train.csv\")\n","test =  pd.read_csv(\"/content/gdrive/MyDrive/XAI/data/medical_tc_test.csv\")\n","labels = pd.read_csv(\"/content/gdrive/MyDrive/XAI/data/medical_tc_labels.csv\")\n","mappings = {\n","    0: \"non-cardiovascular\",\n","    1: \"cardiovascular\"\n","}\n","inverse_mapping = {\n","    \"non-cardiovascular\" : 0,\n","    \"cardiovascular\" : 1\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRbGy9OqiAWB"},"outputs":[],"source":["train['medical_abstract'] = train['medical_abstract'].apply(clean_medical_text)\n","test['medical_abstract'] = test['medical_abstract'].apply(clean_medical_text)\n","train['condition_label'] = train['condition_label'].apply(lambda x: 0 if x in [1,2,3,5] else 1)\n","test['condition_label'] = test['condition_label'].apply(lambda x: 0 if x in [1,2,3,5] else 1)\n","train['condition_label'] = train['condition_label'].apply(lambda x: mappings[x])\n","test['condition_label'] = test['condition_label'].apply(lambda x: mappings[x])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUvi5J_SiAWC"},"outputs":[],"source":["train.to_csv(\"/content/gdrive/MyDrive/XAI/data/medical_tc_train_cleaned.csv\", index=False)\n","test.to_csv(\"/content/gdrive/MyDrive/XAI/data/medical_tc_test_cleaned.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"FmTX1FkViAWD"},"source":["### Fine Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIvG_67-iAWF"},"outputs":[],"source":["train_dataset = MedicalTCDataset(train)\n","test_dataset = MedicalTCDataset(test)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQoaOp2giAWG"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n","model.classifier = nn.Sequential(\n","    nn.Linear(model.config.hidden_size, hidden_dim),\n","    nn.ReLU(),\n","    nn.Dropout(0.2),\n","    nn.Linear(hidden_dim, num_classes)\n",")\n","model.to(device)\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbEIKgWniAWI"},"outputs":[],"source":["model.train()\n","for i, param in enumerate(model.bert.parameters()):\n","    if i < 185:\n","      param.requires_grad = False\n","    else:\n","      param.requires_grad = True\n","for param in model.classifier.parameters():\n","    param.requires_grad = True\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lutw3mMdiAWJ"},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss()\n","train_loss = []\n","model.to(device)\n","model.train()\n","progress_bar = tqdm(range(num_training_steps))\n","for epoch in tqdm(range(num_epochs)):\n","    for text, labels in train_dataloader:\n","        optimizer.zero_grad()\n","        tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=max_length, truncation=True) for v in text]).cuda()\n","        a_mask = (tv!=0).type(torch.int64).cuda()\n","        outputs = model(tv,attention_mask=a_mask)[0].cuda()\n","        labels = list(labels)\n","        labels = [inverse_mapping[l] for l in labels]\n","        labels = torch.Tensor(labels).long().cuda()\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        train_loss.append(loss.item())\n","        optimizer.step()\n","        lr_scheduler.step()\n","        progress_bar.update(1)\n","    if epoch % 10 == 0:\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'loss': loss.item(),\n","        }, '/content/gdrive/MyDrive/XAI/models/biobert_fine_tuned_epoch_{}.pt'.format(epoch))\n","np.save(\"/content/gdrive/MyDrive/XAI/results/biobert_fine_tuned_loss.npy\", np.array(train_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2zV6F28iAWK"},"outputs":[],"source":["num_testing_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_testing_steps\n",")\n","progress_bar = tqdm(range(num_testing_steps))\n","\n","model.eval()\n","eval_loss = []\n","with torch.no_grad():\n","    for text, labels in test_dataloader:\n","        optimizer.zero_grad()\n","        tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=max_length, truncation=True) for v in text]).cuda()\n","        a_mask = (tv!=0).type(torch.int64).cuda()\n","        outputs = model(tv,attention_mask=a_mask)[0].cpu()\n","        labesl = list(labels)\n","        labels = [inverse_mapping[l] for l in labels]\n","        labels = torch.Tensor(labels).long()\n","        loss = criterion(outputs, labels)\n","        eval_loss.append(loss.item())\n","        progress_bar.update(1)\n","average_eval_loss = np.mean(eval_loss)\n","print(f\"Average eval loss: {average_eval_loss}\")\n","np.save(\"/content/gdrive/MyDrive/XAI/results/biobert_fine_tuned_eval_loss.npy\", np.array(eval_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSt0wAc-iAWL"},"outputs":[],"source":["input_json = []\n","for data, text in test_dataloader:\n","    for d in data:\n","      temp = {}\n","      temp['text'] = d\n","      temp['words'] = d.split()\n","      input_json.append(temp)\n","\n","with open('/content/gdrive/MyDrive/XAI/data/input.json', 'w') as f:\n","    json.dump(input_json, f)\n","\n","with open('/content/gdrive/MyDrive/XAI/data/model_config.json', 'w') as f:\n","    json.dump(model.bert.config.to_dict(), f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zodiYGdGv7rm"},"outputs":[],"source":["loss = np.load('/content/gdrive/MyDrive/XAI/data/biobert_fine_tuned_loss.npy')\n","loss = np.mean(loss.reshape(-1, 361), axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6naydDVwBUd"},"outputs":[],"source":["plt.plot(loss, color='red')\n","plt.title('Train loss characteristics')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.grid(True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3cU6pJTvdo7"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}